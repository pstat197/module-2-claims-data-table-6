---
title: "Summary of exploratory tasks"
author: 'Sophie Lian'
date: today
---

### HTML scraping

To determine whether header content improves webpage classification, we extended the HTML scraping process to extract not only paragraph text but also header tags (h1–h6). We applied the same cleaning pipeline to both datasets and used logistic principal component regression (LPCR) to compare predictive performance. Each dataset was transformed using TF–IDF, reduced to 50 principal components, and evaluated with an 80/20 stratified train/test split to ensure a fair comparison.

The results indicate that including header text did not improve predictive accuracy. The LPCR model using only paragraph text achieved an accuracy of 0.547, while the model using both paragraphs and headers decreased slightly to 0.531. This suggests that headers, while potentially informative, were often too inconsistent or sparse across webpages to meaningfully enhance classification. Thus, the additional header text may have introduced noise rather than providing useful signal for the binary prediction task, and therefore we can conclude that including header text did not improve predictive accuracy.

### Bigrams

Do bigrams capture additional information relevant to the classification of interest? Answer the question, **briefly** describe what analysis you conducted to arrive at your answer, and provide quantitative evidence supporting your answer.

Bigrams capture additional information relevant to the classification of interest. To understand if additional information was captured, a unigram model was made as a base case and compared with a stacked bigram model. The models performances were evaluated through test accuracy and AUC. The stacked bigram model has a test accuracy of 0.6472081 and an AUC of 0.6950724 , these are higher than the respective unigram performances of 0.6171285 and 0.6696772. The overall better performance of the stacked bigram model shows that bigrams capture additional information about the claims status of a page.

### Neural net

For preliminary task 3, we trained a feed-forward neural network on the TF-IDF feature matrix generated from the claim text. After removing constant predictors, the final input dimension included several thousand sparse features. The model architecture consisted of three fully connected hidden layers with 64, 32, and 16 units, respectively, each using ReLU activation to capture nonlinear structure in the high-dimensional text data. To reduce overfitting, an important challenge with TF-IDF inputs, we incorporated dropout (0.15) after the first two layers, which randomly deactivates a portion of neurons during training to improve generalization. The final output layer used a single sigmoid unit to produce a probability for the binary classification task.

The network was trained using the Adam optimizer, which adapts learning rates dynamically during training and is well-suited for large, complex deep learning models. We used binary cross-entropy as the loss function, consistent with the binary nature of the prediction target. Training was performed for 25 epochs with a batch size of 32 and included a 20% validation split to monitor performance during training. 

On the test set, the neural network achieved a predictive accuracy of approximately 0.78, which represents a substantial improvement over the baseline logistic principle component regression model from preliminary task 1 that achieved an accuracy of approximately 0.547. This suggests that the neural network was better able to capture the complex, non-linear patterns present in the TF-IDF values of the claims data. Overall, this improvement highlights the value of deeper architectures and regularization when modeling high dimensional text features, leading to stronger predictive performance and more reliable classification claim outcomes. 
